# -*- coding: utf-8 -*-
"""Untitled14.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1N0kIygYesRPal6Yq6GWk2jKCcaNgMNFg

**Import libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.preprocessing import LabelEncoder
from sklearn.model_selection import train_test_split

"""**Loading dataset**"""

data = pd.read_csv('/content/salaries.csv') # Load the dataset from a CSV file

data.duplicated().sum() # Count the number of duplicate rows in the dataset

data.drop_duplicates(inplace=True) #Remove duplicate rows from the dataset in-place

data.isnull().sum() # Check for missing values in each column of the dataset

print(data.dtypes) # Print the data types of each column in the dataset

data

#convert object datatype to string
data['experience_level'] = data['experience_level'].astype('string')
data['employment_type'] = data['employment_type'].astype('string')
data['job_title'] = data['job_title'].astype('string')
data['salary_currency'] = data['salary_currency'].astype('string')
data['employee_residence'] = data['employee_residence'].astype('string')
data['company_location'] = data['company_location'].astype('string')
data['company_size'] = data['company_size'].astype('string')

print(data.dtypes)

data.columns = data.columns.str.strip().str.lower().str.replace(' ', '_') # Clean column names: remove extra spaces, convert to lowercase, and replace spaces with underscores

text_cols = ['experience_level', 'employment_type', 'job_title', 'salary_currency',
             'employee_residence', 'company_location', 'company_size']

for col in text_cols:
    if col in data.columns:
        data[col] = data[col].astype(str).str.strip().str.lower() # Clean selected text columns by stripping whitespace and converting to lowercase



# Create a new column 'same_country' where 1 means the employee and company are in the same country, and 0 otherwise
data['same_country'] = (data['employee_residence'] == data['company_location']).astype(int)

# Create a new column 'is_remote' where 1 means the job is fully remote (remote_ratio = 100), and 0 otherwise
data['is_remote'] = (data['remote_ratio'] == 100).astype(int)

def remove_outliers_iqr(df, column): # Function to remove outliers based on (IQR) method
    # Calculate the first and third quartiles (Q1 and Q3) of the specified column
    Q1 = df[column].quantile(0.25)
    Q3 = df[column].quantile(0.75)
    IQR = Q3 - Q1  # Calculate (IQR)
    # Define the lower and upper bounds for outlier detection
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    return df[(df[column] >= lower_bound) & (df[column] <= upper_bound)] # Filter and return the rows that are within the bounds (not outliers)



"""**Unvariate analysis**"""

# Plot a histogram with a KDE (Kernel Density Estimate) curve to visualize the distribution of salaries in USD
plt.figure()
sns.histplot(data['salary_in_usd'], kde=True, bins=30, color='skyblue')
plt.title("Distribution of Salary in USD")
plt.xlabel("Salary in USD")
plt.ylabel("Frequency")
plt.show()

# Import visualization libraries
import seaborn as sns
import matplotlib.pyplot as plt
# List of columns to plot
columns_to_plot = ['salary', 'salary_in_usd', 'remote_ratio']
# Loop through each column and plot its distribution
for col in columns_to_plot:
    plt.figure(figsize=(8, 4))  # Set figure size
    sns.histplot(data[col], kde=True, bins=30, color='lightgreen')  # Histogram with KDE
    plt.title(f'Distribution of {col}')  # Plot title
    plt.xlabel(col)  # X-axis label
    plt.ylabel('Count')  # Y-axis label
    plt.grid(True)  # Show grid
    plt.tight_layout()  # Adjust layout
    plt.show()  # Display the plot

data.hist(figsize=(15,15))
plt.show()

plt.figure()
sns.histplot(x='employment_type', data=data, palette='Set1')
plt.title("Employment Type Distribution")
plt.xlabel("Employment Type")
plt.ylabel("Count")
plt.show()

plt.figure()
sns.histplot(x='company_size', data=data, palette='cool')
plt.title("Company Size Distribution")
plt.xlabel("Company Size")
plt.ylabel("Count")
plt.show()

"""**Bivariate analysis**"""

# Import visualization libraries
import seaborn as sns
import matplotlib.pyplot as plt
# Set the figure size (width = 10, height = 6 inches)
plt.figure(figsize=(10, 6))
# Create a boxplot showing salary distribution by experience level
sns.boxplot(x='experience_level', y='salary_in_usd', data=data, palette='pastel')
# Set the title of the plot
plt.title('Salary Distribution by Experience Level')
# Label the x-axis
plt.xlabel('Experience Level')
# Label the y-axis
plt.ylabel('Salary in USD')
# Display the plot
plt.show()

# Set the size of the figure (width = 10, height = 6 inches)
plt.figure(figsize=(10, 6))
# Create a scatter plot to show the relationship between remote ratio and salary
# x: percentage of remote work
# y: salary in USD
# data: the DataFrame containing the dataset
# hue: color points based on experience level (en, mi, se, ex)
# palette='Set2': use a color palette with soft, distinguishable colors
sns.scatterplot(x='remote_ratio', y='salary_in_usd', data=data, hue='experience_level', palette='Set2')
# Set the title of the plot
plt.title('Remote Ratio vs Salary')
# Label the x-axis
plt.xlabel('Remote Ratio (%)')
# Label the y-axis
plt.ylabel('Salary in USD')
# Add a legend with a custom title
plt.legend(title='Experience Level')
# Show the plot
plt.show()

# Import necessary libraries for plotting
import matplotlib.pyplot as plt
import seaborn as sns
# Create a new figure with default size
plt.figure()
# Create a bar plot showing the average salary for each experience level
# x: experience level categories (e.g., en, mi, se, ex)
# y: salary values in USD
# data: the DataFrame containing the dataset
# palette="Set2": use a soft color palette for the bars
sns.barplot(x='experience_level', y='salary_in_usd', data=data, palette="Set2")
# Set the title of the chart
plt.title("Salary by Experience Level")
# Label the x-axis
plt.xlabel("Experience Level")
# Label the y-axis
plt.ylabel("Salary in USD")
# Display the plot
plt.show()

# Get the top 10 most common job titles
top_jobs = data['job_title'].value_counts().nlargest(10).index
# Create a new figure
plt.figure()
# Barplot showing average salary for the top 10 job titles
sns.barplot(
    data=data[data['job_title'].isin(top_jobs)],
    x='job_title',
    y='salary_in_usd',
    palette="coolwarm"
)
# Rotate x-axis labels for better readability
plt.xticks(rotation=45, ha='right')
# Set plot title and axis labels
plt.title("Salary by Job Title (Top 10)")
plt.xlabel("Job Title")
plt.ylabel("Salary in USD")
# Show the plot
plt.show()

# Create a new figure
plt.figure()
# Barplot showing average salary for each remote work ratio
sns.barplot(
    x='remote_ratio',
    y='salary_in_usd',
    data=data,
    estimator='mean',  # Use mean salary for each category
    palette="muted"
)
# Set title and axis labels
plt.title("Average Salary by Remote Ratio")
plt.xlabel("Remote Work (%)")
plt.ylabel("Average Salary in USD")
# Show the plot
plt.show()

plt.figure(figsize=(8, 6))
sns.barplot(x='company_size', y='salary_in_usd', data=data, palette='Set2')
plt.title('Salary Distribution by Company Size')
plt.xlabel('Company Size')
plt.ylabel('Salary in USD')
plt.show()

# Set figure size
plt.figure(figsize=(8, 5))
# Create a heatmap of correlations between numeric columns
sns.heatmap(data.select_dtypes(include='number').corr(), annot=True, cmap='coolwarm', fmt=".2f")
# Add a title
plt.title("Correlation Heatmap")
# Show the plot
plt.show()

"""**Apply Encoding**"""

from sklearn.preprocessing import LabelEncoder
# List of categorical columns to encode
categorical_cols = [
    'experience_level',
    'employment_type',
    'job_title',
    'salary_currency',
    'employee_residence',
    'company_location',
    'company_size'
]
# Clean the categorical columns first
for col in categorical_cols:
    if col in data.columns:
        data[col] = data[col].astype(str).str.strip().str.lower()
# Initialize a dictionary to store LabelEncoders
label_encoders = {}
# Fit and transform each categorical column
for col in categorical_cols:
    if col in data.columns:
        le = LabelEncoder()
        data[col] = le.fit_transform(data[col])
        label_encoders[col] = le
        print(f"{col} Classes:", dict(zip(le.classes_, le.transform(le.classes_))))

###data.drop(columns=['salary_currency'], inplace=True)

"""**Modeling**"""

# Import necessary libraries
from sklearn.model_selection import train_test_split  # For splitting data into training and testing sets
from sklearn.ensemble import RandomForestRegressor    # The regression model
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score  # Evaluation metrics

# Define the target variable (the value we want to predict)
y = data['salary_in_usd']

# Define the feature set (independent variables), removing target and redundant currency column
X = data.drop(columns=['salary_in_usd', 'salary_currency'])  # 'salary_currency' is not useful here

# Split the data into training and testing sets (80% train, 20% test)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize the Random Forest Regressor model
model = RandomForestRegressor(random_state=42)

# Train the model using the training data
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)

# Calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))

# Calculate R-squared Score (R²) — indicates how well the model fits the data
r2 = r2_score(y_test, y_pred)

# Print evaluation results
print(f"MAE: {mae:.2f}")          # Lower is better
print("RMSE:", rmse)              # Lower is better
print(f"R^2 Score: {r2:.2f}")     # Closer to 1 is better

# Import the XGBoost library
import xgboost as xgb
# Import regression evaluation metrics from scikit-learn
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
#create model
xgb_model = xgb.XGBRegressor(random_state=42, n_estimators=100)
#Train model
xgb_model.fit(X_train, y_train)
#Prediction
y_pred_xgb = xgb_model.predict(X_test)
# Calculate evaluation metrics for XGBoost predictions
mae = mean_absolute_error(y_test, y_pred_xgb)
rmse = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
r2 = r2_score(y_test, y_pred_xgb)
# Print the evaluation results
print(" XGBoost Evaluation:")
print(f"MAE: {mae:.2f}")
print(f"RMSE: {rmse:.2f}")
print(f"R^2 Score: {r2:.2f}")

# Import evaluation metrics from scikit-learn
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np
# Fit (train) the XGBoost model on the training data
xgb_model.fit(X_train, y_train)
# Make predictions on both training and testing sets
y_pred_train = xgb_model.predict(X_train)
y_pred_test = xgb_model.predict(X_test)
# Define a function to evaluate the model performance
def evaluate(y_true, y_pred, label):
    # Calculate Mean Absolute Error
    mae = mean_absolute_error(y_true, y_pred)
    # Calculate Root Mean Squared Error
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    # Calculate R-squared score
    r2 = r2_score(y_true, y_pred)
    # Print evaluation results with a label (Train/Test)
    print(f"\n{label} Evaluation:")
    print(f"MAE: {mae:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"R^2 Score: {r2:.2f}")
# Run evaluation for both training and testing sets
evaluate(y_train, y_pred_train, "Training Set")
evaluate(y_test, y_pred_test, "Testing Set")

# Import the Gradient Boosting Regressor model and performance metrics
from sklearn.ensemble import GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
import numpy as np

# Initialize the Gradient Boosting Regressor with a fixed random seed
gbr_model = GradientBoostingRegressor(random_state=42)

# Train the model on the training data
gbr_model.fit(X_train, y_train)

# Predict salary values on the test set
y_pred_gbr = gbr_model.predict(X_test)

# Evaluate the model's performance
mae_gbr = mean_absolute_error(y_test, y_pred_gbr)         # Mean Absolute Error
rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))   # Root Mean Squared Error
r2_gbr = r2_score(y_test, y_pred_gbr)                         # R^2 Score

# Print the results
print("Gradient Boosting Evaluation:")
print(f"MAE: {mae_gbr:.2f}")       # Lower is better
print(f"RMSE: {rmse_gbr:.2f}")     # Lower is better
print(f"R^2 Score: {r2_gbr:.2f}")  # Closer to 1 is better

# Import necessary libraries
import numpy as np
import matplotlib.pyplot as plt
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb

# --------------------------------------
# Train Random Forest Regressor
rf_model = RandomForestRegressor(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# Train XGBoost Regressor
xgb_model = xgb.XGBRegressor(random_state=42)
xgb_model.fit(X_train, y_train)
y_pred_xgb = xgb_model.predict(X_test)
# Train Gradient Boosting Regressor
gbr_model = GradientBoostingRegressor(random_state=42)
gbr_model.fit(X_train, y_train)
y_pred_gbr = gbr_model.predict(X_test)
# Calculate Evaluation Metrics for Each Model

# Random Forest metrics
mae_rf = mean_absolute_error(y_test, y_pred_rf)
rmse_rf = np.sqrt(mean_squared_error(y_test, y_pred_rf))
r2_rf = r2_score(y_test, y_pred_rf)
# XGBoost metrics
mae_xgb = mean_absolute_error(y_test, y_pred_xgb)
rmse_xgb = np.sqrt(mean_squared_error(y_test, y_pred_xgb))
r2_xgb = r2_score(y_test, y_pred_xgb)
# Gradient Boosting metrics
mae_gbr = mean_absolute_error(y_test, y_pred_gbr)
rmse_gbr = np.sqrt(mean_squared_error(y_test, y_pred_gbr))
r2_gbr = r2_score(y_test, y_pred_gbr)
# List of model names for plotting
models = ['Random Forest', 'XGBoost', 'Gradient Boosting']
# Plot MAE (Mean Absolute Error)
plt.figure(figsize=(8, 4))
plt.bar(models, [mae_rf, mae_xgb, mae_gbr], color='skyblue')
plt.title('Mean Absolute Error (MAE)')
plt.ylabel('MAE')
# Add value labels above bars
for i, v in enumerate([mae_rf, mae_xgb, mae_gbr]):
    plt.text(i, v + 1000, f"{v:.2f}", ha='center', fontweight='bold')
plt.show()
# Plot RMSE (Root Mean Squared Error)
plt.figure(figsize=(8, 4))
plt.bar(models, [rmse_rf, rmse_xgb, rmse_gbr], color='lightgreen')
plt.title('Root Mean Squared Error (RMSE)')
plt.ylabel('RMSE')
# Add value labels above bars
for i, v in enumerate([rmse_rf, rmse_xgb, rmse_gbr]):
    plt.text(i, v + 1000, f"{v:.2f}", ha='center', fontweight='bold')
plt.show()
# Plot R² Score
plt.figure(figsize=(8, 4))
plt.bar(models, [r2_rf, r2_xgb, r2_gbr], color='salmon')
plt.title('R² Score Comparison')
plt.ylabel('R² Score')
plt.ylim(0, 1)  # Limit y-axis to [0, 1] for R²
# Add value labels above bars
for i, v in enumerate([r2_rf, r2_xgb, r2_gbr]):
    plt.text(i, v + 0.02, f"{v:.2f}", ha='center', fontweight='bold')
plt.show()

# Import XGBoost regression model
from xgboost import XGBRegressor
# Import GridSearchCV for hyperparameter tuning
from sklearn.model_selection import GridSearchCV
# Import evaluation metrics for regression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
# Initialize an XGBoost Regressor with squared error loss and fixed random seed
xgb_model = XGBRegressor(objective='reg:squarederror', random_state=42)
#Select parameter grid
param_grid = {
    'n_estimators': [100, 200],
    'max_depth': [3, 5, 7],
    'learning_rate': [0.01, 0.1, 0.2],
    'subsample': [0.8, 1],
    'colsample_bytree': [0.8, 1]
}

# GridSearchCV
grid_search = GridSearchCV(
    estimator=xgb_model,
    param_grid=param_grid,
    cv=3,
    scoring='neg_mean_squared_error',
    verbose=1,
    n_jobs=-1
)

# Run GridSearchCV to find the best model by fitting it to the training data
grid_search.fit(X_train, y_train)
# Print the best hyperparameters found by GridSearchCV
print("Best Parameters:", grid_search.best_params_)
# Get the best model found during GridSearchCV
best_model = grid_search.best_estimator_
y_pred = best_model.predict(X_test)
#Evaluation
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)
# Print evaluation metrics for the tuned XGBoost model
print("\n Tuned XGBoost Evaluation:")
print(f"MAE: {mae:.2f}") # Mean Absolute Error
print(f"RMSE: {rmse:.2f}") # Root Mean Squared Error
print(f"R^2 Score: {r2:.2f}")  # R-squared (explained variance)

#new dataframe
new_person = pd.DataFrame([{
    'work_year': 2023,
    'experience_level': 2,
    'employment_type': 1,
    'job_title': 4,
    'employee_residence': 5,
    'remote_ratio': 100,
    'company_location': 5,
    'company_size': 1
}])
# Create a new column 'is_remote': 1 if remote_ratio is 100%, else 0
new_person['is_remote'] = new_person['remote_ratio'].apply(lambda x: 1 if x == 100 else 0)
# Create a new column 'same_country': 1 if employee and company are in the same country, else 0
new_person['same_country'] = (new_person['employee_residence'] == new_person['company_location']).astype(int)
# Add a placeholder 'salary' column with value 0 for all rows in 'new_person'
new_person['salary'] = 0
# Reorder or filter 'new_person' DataFrame to keep only the columns used in the model (X)
new_person = new_person[X.columns]
# Prediction by XGBoost
predicted_salary = model.predict(new_person)
print(" Predicted salary in USD:", round(predicted_salary[0], 2))

"""**Deployment**"""

import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import joblib
#Loading dataset
data = pd.read_csv("/content/salaries.csv")
#Categorical columns
categorical_cols = [
    'experience_level', 'employment_type', 'job_title',
    'salary_currency', 'employee_residence', 'company_location', 'company_size'
]
#Cleaning
for col in categorical_cols:
    data[col] = data[col].astype(str).str.strip().str.lower()
#Encoding
label_encoders = {}
for col in categorical_cols:
    le = LabelEncoder()
    data[col] = le.fit_transform(data[col])
    label_encoders[col] = le
#Save encoders
joblib.dump(label_encoders, 'label_encoders.pkl')
# data processing
X = data.drop('salary_in_usd', axis=1)
y = data['salary_in_usd']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
#Train model
model = xgb.XGBRegressor()
model.fit(X_train, y_train)
#Save model
model.save_model("xgb_model.json")

# Import the joblib library for saving and loading models
import joblib
# Save the trained model to a file named "xgb_model.pkl"
joblib.dump(model, "xgb_model.pkl")

def predict_salary(experience_level, employment_type, job_title,
                   salary_currency, employee_residence, company_location,
                   company_size, remote_ratio, company_age):

    # Manual mappings (consistent with what the LabelEncoders were trained on)
    experience_mapping = {"Entry": 0, "Mid": 1, "Senior": 2, "Executive": 3}
    employment_mapping = {"Full-time": 0, "Part-time": 1, "Contract": 2, "Freelance": 3}
    company_size_mapping = {"S": 0, "M": 1, "L": 2}

    # Build the input data dictionary
    input_data = {
        "work_year": [2023],
        "experience_level": [experience_mapping[experience_level]],
        "employment_type": [employment_mapping[employment_type]],
        "job_title": [job_title],
        "employee_residence": [employee_residence],
        "remote_ratio": [remote_ratio],
        "company_location": [company_location],
        "company_size": [company_size_mapping[company_size]],
        "is_remote": [1 if remote_ratio == 100 else 0],
        "same_country": [1 if employee_residence == company_location else 0],
        "company_age": [company_age],
        "salary": [0]
    }

    df = pd.DataFrame(input_data)
    # Encode any remaining categorical columns using the saved LabelEncoders
    for col in df.columns:
        if col in label_encoders:
            df[col] = label_encoders[col].transform(df[col])
    # Ensure the correct column order (matching training set)
    df = df[X.columns]
    # Predict
    predicted_salary = model.predict(df)
    return f" Predicted Salary (USD): {round(predicted_salary[0], 2)}"

import xgboost as xgb
import pickle
#Train model
model = xgb.XGBRegressor()
model.fit(X_train, y_train)
#Save model
with open("xgb_model.pkl", "wb") as f:
    pickle.dump(model, f)

import joblib

# بدل pickle
model = joblib.load("xgb_model.pkl")

import gradio as gr
import pandas as pd
import pickle

# Initialize model
with open("xgb_model.pkl", "rb") as f:
    model = pickle.load(f)

# Columns used in training
features = [
    'work_year', 'experience_level', 'employment_type', 'job_title',
    'salary', 'salary_currency', 'employee_residence', 'remote_ratio',
    'company_location', 'company_size'
]

# Prediction function
def predict_salary(
    work_year, experience_level, employment_type, job_title,
    employee_residence, remote_ratio, company_location, company_size
):
    try:
        # Prepare input data
        new_person = pd.DataFrame([{
            'work_year': work_year,
            'experience_level': experience_level,
            'employment_type': employment_type,
            'job_title': job_title,
            'salary': 0,  # Placeholder value
            'salary_currency': 0,  # Placeholder value
            'employee_residence': employee_residence,
            'remote_ratio': remote_ratio,
            'company_location': company_location,
            'company_size': company_size
        }])

        # Ensure column order
        new_person = new_person[features]

        # Make prediction
        predicted_salary = model.predict(new_person)[0]
        return f"Expected salary in USD: {round(predicted_salary, 2)}"

    except Exception as e:
        return f" An error occurred: {str(e)}"

# Gradio Interface
demo = gr.Interface(
    fn=predict_salary,
    inputs=[
        gr.Number(label="Work Year"),
        gr.Number(label="Experience Level (Encoded)"),
        gr.Number(label="Employment Type (Encoded)"),
        gr.Number(label="Job Title (Encoded)"),
        gr.Number(label="Employee Residence (Encoded)"),
        gr.Number(label="Remote Ratio (0-100)"),
        gr.Number(label="Company Location (Encoded)"),
        gr.Number(label="Company Size (Encoded)")
    ],
    outputs="text",
    title=" Salary Prediction using XGBoost",
    description="Enter the encoded values to predict the salary in USD."
)

demo.launch()